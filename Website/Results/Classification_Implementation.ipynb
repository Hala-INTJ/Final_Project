{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, classification_report\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import re  \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def run_model(X,y,modelName,model):\n",
    "   \n",
    "    # Creating training and testing subsets\n",
    "    split = int(X.shape[0]*0.7)\n",
    "    X_train = X[:split]\n",
    "    X_test = X[split:]\n",
    "    y_train = y[:split]\n",
    "    y_test = y[split:]\n",
    "              \n",
    "    # Standarize the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Fit and evaluate each model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "    results = pd.DataFrame({\n",
    "    \"Prediction\": y_pred, \n",
    "    \"Actual\": y_test\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    CM = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "    d={}\n",
    "    d['Name'] = (f\"{modelName}\")\n",
    "    d['Accuracy Score'] = (f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "    d['Balanced Accuracy Score'] = (f\"{balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "    d['Actual Exceedance, Predicted Exceedance (TP)'] = (f\"{CM[0][0]}\")\n",
    "    d['Actual Exceedance, Predicted Non-Exceedance (FN)'] = (f\"{CM[0][1]}\")\n",
    "    d['Actual Non-Exceedance, Predicted Exceedance (FP)'] = (f\"{CM[1][0]}\")\n",
    "    d['Actual Non-Exceedance, Predicted Non-Exceedance (TN)'] = (f\"{CM[1][1]}\")\n",
    "    d['Actual'] = (f\"{(y_test == 0).sum()}\")\n",
    "\n",
    "\n",
    "    return d, classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(solver='lbfgs'),\n",
    "    'SVC': SVC(kernel='poly'),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'BalancedRandomForest': BalancedRandomForestClassifier(),\n",
    "    'EasyEnsemble': EasyEnsembleClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier()}\n",
    "\n",
    "trains = {\n",
    "    \"9\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P9-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P9.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S17.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S17-TP\"\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P10-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P10.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S18.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S18-TP\"\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P11-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P11.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S19.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S19-TP\"\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P12-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P12.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S20.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S20-TP\"\n",
    "    },\n",
    "    \"13\": {\n",
    "         \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P13-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P13.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S21.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S21-TP\"\n",
    "    },\n",
    "    \"14\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P14-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P14.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S22.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S22-TP\"\n",
    "    }        \n",
    "}    \n",
    "\n",
    "for modelName, model in models.items():\n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    for train, vars in trains.items():\n",
    "        # Connecting to the Database\n",
    "        engine = create_engine(\"postgresql://postgres:postgres@localhost/WWTP\")\n",
    "        conn = engine.connect()\n",
    "\n",
    "        # Reading SQL query into a Dataframe \n",
    "        df_1 = pd.read_sql_query('select * from \"Preliminary\"', con=conn)\n",
    "        df_2 = pd.read_sql_query('select * from \"Primary\"', con=conn)\n",
    "        df_3 = pd.read_sql_query('select * from \"Aeration\"', con=conn)\n",
    "        df_4 = pd.read_sql_query('select * from \"Secondary\"', con=conn)\n",
    "\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "        df_1 = df_1[vars['df_1']] \n",
    "        specific_columns = []\n",
    "        for col in list(df_2.columns):\n",
    "            if (re.match(vars['df_2'], col)):\n",
    "                specific_columns.append(col)         \n",
    "        df_2 = df_2[specific_columns]\n",
    "        specific_columns = []\n",
    "        for col in list(df_3.columns):\n",
    "            if (re.match(vars['df_3'], col)): \n",
    "                specific_columns.append(col)\n",
    "        df_3 = df_3[specific_columns]\n",
    "        specific_columns = []\n",
    "        for col in list(df_4.columns):\n",
    "            if (re.match(vars['df_4'], col)):\n",
    "                specific_columns.append(col)\n",
    "        df_4 = df_4[specific_columns]   \n",
    "\n",
    "        # Merging Dataframes\n",
    "        df_temp_1 = pd.merge(df_1, df_2, on='Time', how='outer')\n",
    "        df_temp_2 = pd.merge(df_temp_1, df_3, on='Time', how='outer')\n",
    "        df = pd.merge(df_temp_2, df_4, on='Time', how='outer')\n",
    "\n",
    "        # Add a classified column for 'TP' - value of 0 for exceedance \"out of compliance\"\n",
    "        df['TP_Exceedance'] = df[vars['target']].apply(lambda x: 1 if x < 0.35 else 0)\n",
    "        df.drop(vars['target'], inplace = True, axis = 1)\n",
    "\n",
    "        # Keeping the records satring on July 1st, 2017\n",
    "        df = df[df['Time'] >= datetime.datetime(2017,7,1)].sort_values(by='Time')\n",
    "\n",
    "        # Resetting the index\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # Dropping columns due to missing data until November 2018\n",
    "        specific_columns = []\n",
    "        for col in df.columns:\n",
    "            if (re.match(r'(^.*-PRI-.*-TKN|^.*-PRI-.*-Ammonia|^.*-PRI-.*-Nitrate|^.*-PRI-.*-Nitrite)', col)):\n",
    "                specific_columns.append(col)\n",
    "        df.drop(columns=specific_columns, inplace = True, axis = 1)            \n",
    "\n",
    "        # Dropping NaN\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Covert Time into numerical columns\n",
    "        df['month'] = df['Time'].dt.month\n",
    "        df['week'] = df['Time'].dt.week\n",
    "        df['day'] = df['Time'].dt.day\n",
    "\n",
    "        # Create a Series for \"Time\" column\n",
    "        time_column = df[\"Time\"]\n",
    "\n",
    "        # Drop the time, year and month columns\n",
    "        df.drop(['Time'], inplace = True, axis = 1)\n",
    "\n",
    "        y = df['TP_Exceedance']\n",
    "        X = df.drop(columns=\"TP_Exceedance\")\n",
    "\n",
    "        outcome, classification = run_model(X,y,modelName,model)\n",
    "\n",
    "        model_results[train] = {\n",
    "            \"outcome\": outcome,\n",
    "            \"classification\": classification\n",
    "        }\n",
    "\n",
    "    fileName = modelName + \"_Classification.json\"\n",
    "    with open(fileName, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(model_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 593us/step - loss: 1.9160e-06 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 508us/step - loss: 1.1905 - accuracy: 0.9831\n",
      "31/31 [==============================] - 0s 504us/step - loss: 4.1367e-06 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 532us/step - loss: 0.9274 - accuracy: 0.9564\n",
      "31/31 [==============================] - 0s 463us/step - loss: 2.3902e-05 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 567us/step - loss: 2.1181 - accuracy: 0.8789\n",
      "31/31 [==============================] - 0s 552us/step - loss: 3.7895e-05 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.3625 - accuracy: 0.9927\n",
      "31/31 [==============================] - 0s 469us/step - loss: 3.7524e-05 - accuracy: 1.0000\n",
      "13/13 [==============================] - 0s 542us/step - loss: 6.1472 - accuracy: 0.9007\n",
      "18/18 [==============================] - 0s 504us/step - loss: 5.4533e-06 - accuracy: 1.0000\n",
      "8/8 [==============================] - 0s 575us/step - loss: 4.0456 - accuracy: 0.9426\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import re  \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def neural_model(X, y, layer_nodes, activation, epochs):\n",
    "   \n",
    "    # Creating training and testing subsets\n",
    "    split = int(X.shape[0]*0.7)\n",
    "    X_train = X[:split]\n",
    "    X_test = X[split:]\n",
    "    y_train = y[:split]\n",
    "    y_test = y[split:]\n",
    "              \n",
    "    # Standarize the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "    model = Sequential()\n",
    "    for index, nodes in enumerate(layer_nodes):\n",
    "        if index == 0:\n",
    "            model.add(Dense(units=nodes, input_dim=len(X_train_scaled[0]), activation=activation))\n",
    "        else:\n",
    "            model.add(Dense(units=nodes, activation=activation))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Check the structure of the model\n",
    "    # print(model.summary())\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),epochs=epochs,verbose=0)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred = pd.Series(pd.DataFrame.from_records(y_pred)[0].values)\n",
    "    \n",
    "    \n",
    "    metrics = pd.DataFrame({\n",
    "        \"Train Loss\": history.history['loss'],\n",
    "        \"Test Loss\": history.history['val_loss'],\n",
    "        \"Train Accuracy\": history.history['accuracy'],\n",
    "        \"Test Accuracy\": history.history['val_accuracy']\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    model_loss_train, model_accuracy_train = model.evaluate(X_train_scaled,y_train)\n",
    "    model_loss_test, model_accuracy_test = model.evaluate(X_test_scaled,y_test)\n",
    "    \n",
    "    d={}\n",
    "    d['Name'] = (f\"Neural Model: {layer_nodes}, {activation}, {epochs}\")\n",
    "    d['Accuracy Score Train'] = (f\"{model_accuracy_train:.4f}\")\n",
    "    d['Accuracy Score Test'] = (f\"{model_accuracy_test:.4f}\")\n",
    "    d['Model Loss Train'] = (f\"{model_loss_train:.4f}\")\n",
    "    d['Model Loss Test'] = (f\"{model_loss_test:.4f}\")\n",
    "    d['Predicted Correctly'] = (f\"{((y_pred == 0) & (y_test == 0)).sum()}\")\n",
    "    d['Actual'] = (f\"{(y_test == 0).sum()}\")\n",
    "\n",
    "\n",
    "    return d, metrics.to_dict()\n",
    "\n",
    "trains = {\n",
    "    \"9\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P9-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P9.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S17.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S17-TP\"\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P10-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P10.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S18.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S18-TP\"\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P11-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P11.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S19.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S19-TP\"\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P12-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P12.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S20.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S20-TP\"\n",
    "    },\n",
    "    \"13\": {\n",
    "         \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P13-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P13.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S21.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S21-TP\"\n",
    "    },\n",
    "    \"14\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P14-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P14.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S22.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S22-TP\"\n",
    "    }        \n",
    "}    \n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for train, vars in trains.items():\n",
    "    # Connecting to the Database\n",
    "    engine = create_engine(\"postgresql://postgres:postgres@localhost/WWTP\")\n",
    "    conn = engine.connect()\n",
    "\n",
    "    # Reading SQL query into a Dataframe \n",
    "    df_1 = pd.read_sql_query('select * from \"Preliminary\"', con=conn)\n",
    "    df_2 = pd.read_sql_query('select * from \"Primary\"', con=conn)\n",
    "    df_3 = pd.read_sql_query('select * from \"Aeration\"', con=conn)\n",
    "    df_4 = pd.read_sql_query('select * from \"Secondary\"', con=conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    df_1 = df_1[vars['df_1']] \n",
    "    specific_columns = []\n",
    "    for col in list(df_2.columns):\n",
    "        if (re.match(vars['df_2'], col)):\n",
    "            specific_columns.append(col)         \n",
    "    df_2 = df_2[specific_columns]\n",
    "    specific_columns = []\n",
    "    for col in list(df_3.columns):\n",
    "        if (re.match(vars['df_3'], col)): \n",
    "            specific_columns.append(col)\n",
    "    df_3 = df_3[specific_columns]\n",
    "    specific_columns = []\n",
    "    for col in list(df_4.columns):\n",
    "        if (re.match(vars['df_4'], col)):\n",
    "            specific_columns.append(col)\n",
    "    df_4 = df_4[specific_columns]   \n",
    "\n",
    "    # Merging Dataframes\n",
    "    df_temp_1 = pd.merge(df_1, df_2, on='Time', how='outer')\n",
    "    df_temp_2 = pd.merge(df_temp_1, df_3, on='Time', how='outer')\n",
    "    df = pd.merge(df_temp_2, df_4, on='Time', how='outer')\n",
    "\n",
    "    # Add a classified column for 'TP' - value of 0 for exceedance \"out of compliance\"\n",
    "    df['TP_Exceedance'] = df[vars['target']].apply(lambda x: 1 if x < 0.35 else 0)\n",
    "    df.drop(vars['target'], inplace = True, axis = 1)\n",
    "\n",
    "    # Keeping the records satring on July 1st, 2017\n",
    "    df = df[df['Time'] >= datetime.datetime(2017,7,1)].sort_values(by='Time')\n",
    "\n",
    "    # Resetting the index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Dropping columns due to missing data until November 2018\n",
    "    specific_columns = []\n",
    "    for col in df.columns:\n",
    "        if (re.match(r'(^.*-PRI-.*-TKN|^.*-PRI-.*-Ammonia|^.*-PRI-.*-Nitrate|^.*-PRI-.*-Nitrite)', col)):\n",
    "            specific_columns.append(col)\n",
    "    df.drop(columns=specific_columns, inplace = True, axis = 1)            \n",
    "\n",
    "    # Dropping NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Covert Time into numerical columns\n",
    "    df['month'] = df['Time'].dt.month\n",
    "    df['week'] = df['Time'].dt.week\n",
    "    df['day'] = df['Time'].dt.day\n",
    "\n",
    "    # Create a Series for \"Time\" column\n",
    "    time_column = df[\"Time\"]\n",
    "\n",
    "    # Drop the time, year and month columns\n",
    "    df.drop(['Time'], inplace = True, axis = 1)\n",
    "\n",
    "    y = df['TP_Exceedance']\n",
    "    X = df.drop(columns=\"TP_Exceedance\")\n",
    "\n",
    "    layer_nodes=[100,75]\n",
    "    activation='relu'\n",
    "    epochs=500\n",
    "    \n",
    "    outcome, metrics =  neural_model(X, y, layer_nodes, activation, epochs)\n",
    "    \n",
    "    model_results[train] = {\n",
    "        \"outcome\": outcome,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "fileName = \"Neural_Network_Classification.json\"\n",
    "with open(fileName, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(model_results, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

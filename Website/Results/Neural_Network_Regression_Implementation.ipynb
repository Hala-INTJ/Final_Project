{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def neural_model(X, y, layer_nodes, activation, epochs):\n",
    "\n",
    "   # Creating training and testing subsets\n",
    "    split = int(X.shape[0]*0.7)\n",
    "    X_train = X[:split]\n",
    "    X_test = X[split:]\n",
    "    y_train = y[:split]\n",
    "    y_test = y[split:]\n",
    "\n",
    "    # Standarize the data\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    Y_scaler = StandardScaler().fit(pd.DataFrame(y_train))\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    y_train_scaled = Y_scaler.transform(pd.DataFrame(y_train))\n",
    "    y_test_scaled = Y_scaler.transform(pd.DataFrame(y_test))\n",
    "\n",
    "    # Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "    model = Sequential()\n",
    "    for index, nodes in enumerate(layer_nodes):\n",
    "        if index == 0:\n",
    "            model.add(Dense(units=nodes, input_dim=len(\n",
    "                X_train_scaled[0]), activation=activation))\n",
    "        else:\n",
    "            model.add(Dense(units=nodes, activation=activation))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    # # Check the structure of the model\n",
    "    # print(model.summary())\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"mean_squared_error\",\n",
    "                  optimizer=\"adam\", metrics=['mse', 'mae'])\n",
    "\n",
    "    model.fit(X_train_scaled, y_train_scaled, epochs=epochs, verbose=0)\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "    y_test = Y_scaler.inverse_transform(pd.DataFrame(y_test_scaled))\n",
    "    y_pred = Y_scaler.inverse_transform(pd.DataFrame(y_pred_scaled))\n",
    "\n",
    "    y_test = pd.Series(pd.DataFrame.from_records(y_test)[0].values)\n",
    "    y_pred = pd.Series(pd.DataFrame.from_records(y_pred)[0].values)\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"Prediction\": y_pred,\n",
    "        \"Actual\": y_test\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    adjusted_r2 = 1-(1-(r2_score(y_test, y_pred)))*((len(X_test_scaled)-1)\n",
    "                                                    )/(len(X_test_scaled)-len(X_test_scaled[0])-1)\n",
    "\n",
    "    d = {}\n",
    "    d['Name'] = 'Neural_Network_Regression'\n",
    "    d['R2'] = (f\"{r2_score(y_test, y_pred):.4f}\")\n",
    "    d['Adjusted R2'] = (f\"{adjusted_r2:.4f}\")\n",
    "    d['Mean Square Error'] = (f\"{mean_squared_error(y_test, y_pred):.4f}\")\n",
    "    d['Root Mean Square Error'] = (\n",
    "        f\"{math.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "    d['Mean Absolute Error'] = (f\"{mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "    d['Predicted Correctly'] = (f\"{((y_pred > 0.35) & (y_test > 0.35)).sum()}\")\n",
    "    d['Actual > 0.35'] = (f\"{(y_test > 0.35).sum()}\")\n",
    "\n",
    "    return d, results.to_dict()\n",
    "\n",
    "\n",
    "trains = {\n",
    "    \"9\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P9-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P9.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S17.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S17-TP\"\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P10-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P10.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S18.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S18-TP\"\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"df_1\": ['Time', 'T5-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T5.*-P11-.*|Time)',\n",
    "        \"df_3\": r'(^T5.*-P11.*|Time)',\n",
    "        \"df_4\": r'(^T5.*-S19.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S19-TP\"\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P12-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P12.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S20.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S20-TP\"\n",
    "    },\n",
    "    \"13\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P13-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P13.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S21.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S21-TP\"\n",
    "    },\n",
    "    \"14\": {\n",
    "        \"df_1\": ['Time', 'T6-S3-PRE-FeCL2'],\n",
    "        \"df_2\": r'(^T6.*-P14-.*|Time)',\n",
    "        \"df_3\": r'(^T6.*-P14.*|Time)',\n",
    "        \"df_4\": r'(^T[5,6].*-S22.*|Time)',\n",
    "        \"target\": \"T5-S3-SEC-S22-TP\"\n",
    "    }\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for train, vars in trains.items():\n",
    "    # Connecting to the Database\n",
    "    engine = create_engine(\"postgresql://postgres:postgres@localhost/WWTP\")\n",
    "    conn = engine.connect()\n",
    "\n",
    "    # Reading SQL query into a Dataframe\n",
    "    df_1 = pd.read_sql_query('select * from \"Preliminary\"', con=conn)\n",
    "    df_2 = pd.read_sql_query('select * from \"Primary\"', con=conn)\n",
    "    df_3 = pd.read_sql_query('select * from \"Aeration\"', con=conn)\n",
    "    df_4 = pd.read_sql_query('select * from \"Secondary\"', con=conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    df_1 = df_1[vars['df_1']]\n",
    "    specific_columns = []\n",
    "    for col in list(df_2.columns):\n",
    "        if (re.match(vars['df_2'], col)):\n",
    "            specific_columns.append(col)\n",
    "    df_2 = df_2[specific_columns]\n",
    "    specific_columns = []\n",
    "    for col in list(df_3.columns):\n",
    "        if (re.match(vars['df_3'], col)):\n",
    "            specific_columns.append(col)\n",
    "    df_3 = df_3[specific_columns]\n",
    "    specific_columns = []\n",
    "    for col in list(df_4.columns):\n",
    "        if (re.match(vars['df_4'], col)):\n",
    "            specific_columns.append(col)\n",
    "    df_4 = df_4[specific_columns]\n",
    "\n",
    "    # Merging Dataframes\n",
    "    df_temp_1 = pd.merge(df_1, df_2, on='Time', how='outer')\n",
    "    df_temp_2 = pd.merge(df_temp_1, df_3, on='Time', how='outer')\n",
    "    df = pd.merge(df_temp_2, df_4, on='Time', how='outer')\n",
    "\n",
    "    # Keeping the records satring on July 1st, 2017\n",
    "    df = df[df['Time'] >= datetime.datetime(\n",
    "        2017, 7, 1)].sort_values(by='Time')\n",
    "\n",
    "    # Resetting the index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Dropping columns due to missing data until November 2018\n",
    "    specific_columns = []\n",
    "    for col in df.columns:\n",
    "        if (re.match(r'(^.*-PRI-.*-TKN|^.*-PRI-.*-Ammonia|^.*-PRI-.*-Nitrate|^.*-PRI-.*-Nitrite)', col)):\n",
    "            specific_columns.append(col)\n",
    "    df.drop(columns=specific_columns, inplace=True, axis=1)\n",
    "\n",
    "    # Dropping NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Covert Time into numerical columns\n",
    "    df['month'] = df['Time'].dt.month\n",
    "    df['week'] = df['Time'].dt.week\n",
    "    df['day'] = df['Time'].dt.day\n",
    "\n",
    "    # Create a Series for \"Time\" column\n",
    "    time_column = df[\"Time\"]\n",
    "\n",
    "    # Drop the time, year and month columns\n",
    "    df.drop(['Time'], inplace=True, axis=1)\n",
    "\n",
    "    y = df[vars['target']]\n",
    "    X = df.drop(columns=vars['target'])\n",
    "\n",
    "    layer_nodes=[100,75]\n",
    "    activation='linear'\n",
    "    epochs=500\n",
    "\n",
    "    outcome, results = neural_model(X, y, layer_nodes, activation, epochs)\n",
    "\n",
    "    model_results[train] = {\n",
    "        \"outcome\": outcome,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "fileName = \"Neural_Network_Regression.json\"\n",
    "with open(fileName, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(model_results, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
